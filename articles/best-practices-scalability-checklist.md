---
title: "Liste de contrôle de l’extensibilité | Microsoft Docs"
description: "Liste de contrôle de l’extensibilité et recommandations concernant les problématiques de conception pour la mise à l’échelle automatique d’Azure."
services: 
documentationcenter: na
author: dragon119
manager: christb
editor: 
tags: 
ms.assetid: e505c665-a095-4013-a4b6-ccd79bcb2e1e
ms.service: best-practice
ms.devlang: na
ms.topic: article
ms.tgt_pltfrm: na
ms.workload: na
ms.date: 07/13/2016
ms.author: masashin
translationtype: Human Translation
ms.sourcegitcommit: 219dcbfdca145bedb570eb9ef747ee00cc0342eb
ms.openlocfilehash: eeb06f2d0f20b043728fe6daacbfbaa9165cb01d


---
# <a name="scalability-checklist"></a>Liste de contrôle de l’extensibilité
[!INCLUDE [pnp-header](../includes/guidance-pnp-header-include.md)]

## <a name="service-design"></a>Conception du service
* **Partitionnez la charge de travail**. Concevez les parties du processus pour qu’elles soient distinctes et décomposables. Minimisez la taille de chaque partie tout en respectant les règles habituelles de séparation des éléments problématiques et le principe de responsabilité unique. Cela permet de distribuer les parties du composant de façon à optimiser l’utilisation de chaque unité de calcul (par exemple un serveur de bases de données ou de rôles). Cela facilite également la mise à l’échelle de l’application grâce à l’ajout d’instances de ressources spécifiques. Pour plus d’informations, consultez la page [Recommandations en matière de partitionnement des unités de calcul](https://msdn.microsoft.com/library/dn589773.aspx).
* **Pensez la conception pour la mise à l’échelle**. La mise à l’échelle permet aux applications de répondre à la variabilité de la charge en augmentant ou en diminuant le nombre d’instances de rôles, de files d’attente et d’autres services qu’elles utilisent. L’application doit toutefois être conçue en ce sens. Par exemple, l’application et les services qu’elle utilise doivent être sans état, pour que les demandes puissent être acheminées vers n’importe quelle instance. Cela empêche également que l’ajout ou la suppression d’instances spécifiques aient un impact négatif sur les utilisateurs actuels. Vous devez aussi implémenter la configuration ou la détection automatique des instances à mesure qu’elles sont ajoutées et supprimées, pour que le code de l’application puisse effectuer le routage nécessaire. Par exemple, une application web peut utiliser un ensemble de files d’attente selon une approche de type tourniquet (round robin) pour acheminer les demandes vers les services d’arrière-plan exécutés dans les rôles de travail. L’application web doit pouvoir détecter les modifications du nombre de files d’attente, pour pouvoir acheminer correctement les demandes et équilibrer la charge sur l’application.
* **Considérez la mise à l’échelle de manière unitaire**. Prévoyez des ressources supplémentaires pour prendre en charge la croissance. Pour chaque ressource, vous devez connaître les limites supérieures de mise à l’échelle et utiliser le partitionnement ou la décomposition pour dépasser ces limites. Déterminez les unités d’échelle pour le système en termes de jeux bien définis de ressources. Ainsi, la mise en œuvre des opérations de montée en charge est plus simple et moins susceptible d’avoir des conséquences néfastes sur l’application à cause de limitations imposées par le manque de ressources dans une partie du système global. Par exemple, l’ajout de x rôles de travail et web peut nécessiter y files d’attente supplémentaires et z comptes de stockage pour gérer la charge de travail supplémentaire générée par les rôles. Une unité d’échelle peut donc comprendre x rôles de travail et web, *y* files d’attente et *z* comptes de stockage. Concevez l’application pour qu’elle puisse facilement être mise à l’échelle en ajoutant une ou plusieurs unités d’échelle.
* **Évitez l’affinité du client**. Si possible, assurez-vous que l’application ne nécessite pas d’affinité. Les demandes peuvent ainsi être acheminées vers n’importe quelle instance, et le nombre d’instances importe peu. Cela élimine également la surcharge liée au stockage, à la récupération et à la maintenance des informations d’état pour chaque utilisateur.
* **Tirez parti des fonctionnalités de mise à l’échelle automatique de la plateforme**. Quand la plateforme d’hébergement prend en charge une capacité de mise à l’échelle automatique, telle que la fonction Mise à l’échelle automatique d’Azure, préférez celle-ci aux mécanismes personnalisés ou tiers, à moins que le mécanisme intégré ne puisse pas satisfaire à vos exigences. Utilisez si possible des règles de mise à l’échelle planifiées pour vous assurer que les ressources sont disponibles sans délai de démarrage, mais ajoutez si nécessaire une mise à l’échelle automatique réactive aux règles pour faire face aux évolutions imprévues de la demande. Vous pouvez utiliser les opérations de mise à l’échelle automatique de l’API Service Management pour ajuster la mise à l’échelle automatique et pour ajouter des compteurs personnalisés aux règles. Pour plus d’informations, consultez [Recommandations en matière de mise à l’échelle automatique](best-practices-auto-scaling.md).
* **Déchargez les tâches utilisant l’UC et les E/S de manière intensive en tant que tâches en arrière-plan**. Si vous vous attendez à ce qu’une demande à un service mette beaucoup de temps à être exécutée ou absorbe des ressources considérables, déchargez le traitement de cette demande vers une tâche distincte. Utilisez des rôles de travail ou des travaux en arrière-plan (selon la plate-forme d’hébergement) pour exécuter ces tâches. Cette stratégie permet au service de continuer à recevoir des demandes et de rester réactif.  Pour plus d’informations, consultez le [Guide relatif aux travaux en arrière-plan](best-practices-background-jobs.md).
* **Distribuez la charge de travail pour les tâches en arrière-plan**. Quand les tâches en arrière-plan sont nombreuses ou nécessitent beaucoup de temps ou de ressources, répartissez la charge de travail entre plusieurs unités de calcul (par exemple, des rôles de travail ou des travaux en arrière-plan). Le [modèle des Consommateurs concurrents](https://msdn.microsoft.com/library/dn568101.aspx)offre une solution possible.
* **Envisagez la migration vers une architecture** *sans partage*. Une architecture sans partage utilise des nœuds indépendants et autonomes qui n’ont aucun point de contention (tel que des services partagés ou du stockage). En théorie, un tel système peut être mis à l’échelle presque indéfiniment. Bien qu’une approche sans le moindre partage ne soit généralement pas pratique pour la plupart des applications, elle peut offrir des opportunités de conception avec une meilleure extensibilité. Par exemple, éviter l’utilisation de l’état de session, de l’affinité du client et du partitionnement des données côté serveur est un bon exemple de migration vers une architecture sans partage.

## <a name="data-management"></a>Gestion des données
* **Utilisez le partitionnement des données**. Répartissez les données entre plusieurs bases de données et serveurs de bases de données, ou concevez l’application pour qu’elle utilise des services de stockage de données à même de fournir ce partitionnement en toute transparence (par exemple, la base de données élastique Azure SQL Database ou le stockage de tables Azure). Cette approche peut contribuer à optimiser les performances et à faciliter la mise à l’échelle. Il existe différentes techniques de partitionnement (horizontal, vertical et fonctionnel). Vous pouvez associer ces techniques pour tirer le meilleur parti des performances de requête accrues, de l’extensibilité simplifiée, de la gestion plus flexible et de la disponibilité améliorée, tout en faisant correspondre le type de magasin aux données qu’il contiendra. Envisagez également d’utiliser différents types de magasin de données pour les différents types de données, en choisissant ceux qui sont le mieux adaptés aux types de données spécifiques. Cela peut impliquer l’utilisation d’une table de stockage, d’une base de données de documents ou d’un magasin de données colonne-famille au lieu ou en plus d’une base de données relationnelle. Pour plus d’informations, consultez la page [Recommandations en matière de partitionnement](best-practices-data-partitioning.md).
* **Pensez la conception pour la cohérence finale**. La cohérence finale améliore l’extensibilité en réduisant ou en éliminant le temps nécessaire à la synchronisation des données associées partitionnées entre plusieurs magasins. Le problème est que les données ne sont pas toujours cohérentes lorsqu’elles sont lues et que certaines opérations d’écriture peuvent provoquer des conflits. La cohérence finale est idéale pour les situations où les mêmes données sont fréquemment lues mais rarement écrites. Pour plus d’informations, consultez les [conseils en matière de cohérence des données](https://msdn.microsoft.com/library/dn589800.aspx).
* **Réduisez les interactions impliquant de nombreux échanges entre les composants et les services**. Évitez de concevoir des interactions dans lesquelles une application doit effectuer plusieurs appels à un service (chaque appel renvoyant une petite quantité de données) plutôt qu’un appel unique pouvant renvoyer l’ensemble des données. Si possible, associez plusieurs opérations connexes dans une seule demande lorsque l’appel effectué est à destination d’un service ou d’un composant présentant une latence importante. Cela simplifie la surveillance des performances et l’optimisation des opérations complexes. Par exemple, utilisez des procédures stockées dans les bases de données pour encapsuler une logique complexe et réduire le nombre d’allers et retours et le verrouillage de ressources.
* **Utilisez des files d’attente afin de niveler la charge pour les écritures de données à haute vitesse**. Les pics de demande pour un service peuvent surcharger ce service et provoquer des défaillances toujours plus importantes. Pour éviter ce problème, envisagez d’implémenter le [modèle de nivellement de charge basé sur la file d’attente](https://msdn.microsoft.com/library/dn589783.aspx). Utilisez une file d’attente qui agit comme mémoire tampon entre une tâche et un service qu’elle appelle. Cela permet d’atténuer les surcharges intermittentes qui pourraient autrement entraîner la défaillance du service ou l’expiration de la tâche.
* **Réduisez la charge sur le magasin de données**. Le magasin de données constitue généralement un goulot d’étranglement, une ressource coûteuse et s’avère souvent complexe à monter en charge. Si possible, supprimez la logique (telle que le traitement des documents XML ou des objets JSON) du magasin de données et effectuez le traitement dans l’application. Par exemple, au lieu de transmettre le XML à la base de données (autrement que comme une chaîne opaque pour le stockage), sérialisez ou désérialisez le XML dans la couche application et transmettez-le dans un format natif au magasin de données. Il est généralement plus facile de monter en charge une application que le magasin de données. Ainsi, vous devez essayer d’effectuer la plus grande partie du traitement nécessitant beaucoup de ressources au sein de l’application.
* **Réduisez le volume de données récupérées**. Récupérez uniquement les données dont vous avez besoin en spécifiant des colonnes et en utilisant des critères pour sélectionner les lignes. Exploitez les paramètres de valeur de table et le niveau d’isolement approprié. Utilisez des mécanismes tels que des étiquettes d’entités pour éviter de récupérer des données inutiles.
* **Utilisez la mise en cache de manière intensive**. Utilisez la mise en cache autant que possible pour réduire la charge sur les ressources et les services qui génèrent ou fournissent des données. La mise en cache est généralement adaptée aux données relativement statiques ou dont l’obtention nécessite un traitement considérable. La mise en cache doit intervenir à tous les niveaux appropriés dans chaque couche de l’application, y compris l’accès aux données et la génération de l’interface utilisateur. Pour plus d’informations, consultez la page [Conseils de mise en cache](best-practices-caching.md).
* **Gérez la croissance et la rétention des données**. La quantité de données stockées par une application augmente au fil du temps. Cette croissance entraîne l’augmentation des coûts de stockage et de la latence lors de l’accès aux données, ce qui affecte le rendement et les performances de l’application. Il peut être possible d’archiver certaines données anciennes qui ne sont plus utilisées ou de déplacer les données rarement utilisées dans un stockage à long terme plus économique, même si la latence d’accès est supérieure.
* **Optimisez les objets de transfert de données à l’aide d’un format binaire efficace**. Les objets de transfert de données sont passés entre les couches d’une application à de nombreuses reprises. Réduire la taille permet de réduire la charge sur les ressources et le réseau. Cependant, vous devez comparer les économies réalisées à la surcharge liée à la conversion des données au format requis dans chaque emplacement où elles sont utilisées. Adoptez un format offrant une interopérabilité maximale pour faciliter la réutilisation d’un composant.
* **Définissez le contrôle de cache**. Concevez et configurez l’application pour qu’elle utilise si possible la mise en cache de sortie ou la mise en cache de fragments, pour réduire la charge de traitement.
* **Activez la mise en cache côté client**. Les applications web doivent activer les paramètres de cache pour le contenu pouvant être mis en cache. Ces paramètres sont généralement désactivés par défaut. Configurez le serveur de sorte qu’il fournisse les en-têtes de contrôle de cache appropriés pour activer la mise en cache du contenu sur les serveurs proxy et les clients.
* **Utilisez le stockage d’objets blob Azure et le réseau de distribution de contenu Azure pour réduire la charge sur l’application**. Stockez le contenu public statique ou relativement statique tel que les images, les ressources, les scripts et les feuilles de style dans le stockage d’objets blob. Cette approche soulage l’application de la charge causée par la génération dynamique de ce contenu pour chaque demande. Vous pouvez également utiliser le réseau de distribution de contenu pour mettre en cache ce contenu et le remettre aux clients. L’utilisation du réseau de distribution de contenu peut améliorer les performances au niveau du client, car le contenu est distribué à partir du centre de données contenant un cache de réseau de distribution de contenu le plus proche géographiquement. Pour plus d’informations, consultez l’ [Aide relative au réseau de distribution de contenu (CDN)](best-practices-cdn.md).
* **Optimisez et ajustez les requêtes et index SQL**. Certaines instructions ou constructions T-SQL sont susceptibles d’avoir un impact sur les performances, qui peut être réduit en optimisant le code dans une procédure stockée. Par exemple, évitez de convertir des types **datetime** en **varchar** avant de les comparer à une valeur littérale **datetime**. Utilisez plutôt des fonctions de comparaison de date/heure. L’absence d’index appropriés peut également ralentir l’exécution des requêtes. Si vous utilisez une infrastructure de mappage objet/relationnel, examinez son fonctionnement et l’impact qu’elle peut avoir sur les performances de la couche d’accès aux données. Pour plus d’informations, consultez la page [Ajustement des requêtes](https://technet.microsoft.com/library/ms176005.aspx).
* **Envisagez de dénormaliser les données**. La normalisation des données permet d’éviter la duplication et les incohérences. Cependant, la maintenance de plusieurs index, la vérification de l’intégrité référentielle, les accès multiples à de petits blocs de données et la jointure des tables pour rassembler les données imposent une surcharge qui peut affecter les performances. Demandez-vous si un volume de stockage et une duplication supplémentaires sont acceptables pour réduire la charge sur le magasin de données. Demandez-vous également si l’application proprement dite (qui est généralement plus facile à mettre à l’échelle) offre la fiabilité nécessaire pour prendre le relais dans l’exécution de tâches telles que la gestion de l’intégrité référentielle afin de réduire la charge sur le magasin de données. Pour plus d’informations, consultez la page [Recommandations en matière de partitionnement](https://github.com/mspnp/azure-guidance/blob/master/Data%20partitioning.md).

## <a name="service-implementation"></a>Implémentation du service
* **Utilisez des appels asynchrones**. Utilisez autant que possible du code asynchrone lors de l’accès à des ressources ou des services susceptibles d’être limités par les E/S ou la bande passante réseau, ou qui présentent une latence importance, pour éviter le verrouillage du thread appelant. Pour implémenter des opérations asynchrones, utilisez le [modèle asynchrone basé sur des tâches](https://msdn.microsoft.com/library/hh873175.aspx).
* **Évitez le verrouillage des ressources et utilisez plutôt une approche optimiste**. Ne verrouillez jamais l’accès aux ressources telles que le stockage ou d’autres services présentant une latence importante, car il s’agit d’une des principales causes de faibles performances. Utilisez toujours des approches optimistes pour la gestion des opérations simultanées, telles que l’écriture dans le stockage. Utilisez les fonctionnalités de la couche de stockage pour gérer les conflits. Dans les applications distribuées, il se peut que les données ne soient cohérentes qu’à la fin.
* **Compressez les données hautement compressibles sur les réseaux à latence élevée et faible bande passante**. Dans la plupart des cas dans une application web, le volume le plus important de données générées par l’application et transmises sur le réseau correspond aux réponses HTTP aux demandes des clients. La compression HTTP peut réduire considérablement ce volume, notamment pour le contenu statique. Cela peut réduire les coûts et diminuer la charge sur le réseau, même si la compression de contenu dynamique applique une charge légèrement supérieure sur le serveur. Dans d’autres environnements plus généralisés, la compression de données peut réduire le volume de données transmises et minimiser le temps et les coûts de transfert, mais les processus de compression et de décompression entraînent des frais supplémentaires. Ainsi, la compression ne doit être utilisée que lorsque le gain de performances est avéré. D’autres méthodes de sérialisation (encodage JSON ou binaire, par exemple) peuvent réduire la taille de charge utile tout en ayant moins d’impact sur les performances, tandis que XML est susceptible de l’augmenter.
* **Réduisez le temps d’utilisation des connexions et des ressources**. Maintenez les connexions et les ressources uniquement le temps de les utiliser. Par exemple, ouvrez les connexions le plus tard possible et autorisez leur retour dans le pool de connexions le plus tôt possible. Faites l’acquisition des ressources le plus tard possible et libérez-les le plus tôt possible.
* **Réduisez le nombre de connexions requises**. Les connexions au service absorbent les ressources. Limitez le nombre de connexions nécessaires et assurez-vous que les connexions existantes sont réutilisées dans la mesure du possible. Par exemple, après l’exécution de l’authentification, utilisez l’emprunt d’identité là où cela est nécessaire pour exécuter du code sous une identité spécifique. Cela peut contribuer à optimiser l’utilisation du pool de connexions en réutilisant les connexions.
  
  > [!NOTE]
  > : les API de certains services réutilisent automatiquement les connexions, à condition que les directives propres aux services soient suivies. Il est important de comprendre les conditions qui permettent la réutilisation de la connexion pour chaque service utilisé par votre application.
  > 
  > 
* **Envoyez des demandes par lots pour optimiser l’utilisation du réseau**. Par exemple, envoyez et lisez des messages par lots lorsque vous accédez à une file d’attente et effectuez plusieurs lectures ou écritures par lot lors de l’accès au stockage ou à un cache. Cela peut contribuer à optimiser l’efficacité des services et des magasins de données en réduisant le nombre d’appels sur le réseau.
* **Éliminez si possible la nécessité de stocker l’état de session côté serveur** . La gestion de l’état de session côté serveur nécessite généralement l’affinité du client (autrement dit, le routage de chaque demande vers la même instance de serveur), ce qui affecte la capacité de mise à l’échelle du système. Dans l’idéal, vous devez concevoir les clients pour qu’ils soient sans état par rapport aux serveurs qu’ils utilisent. Cependant, si l’application doit tenir à jour l’état de session, stockez les données sensibles ou les volumes importants de données par client dans un cache distribué côté serveur auquel toutes les instances de l’application peuvent accéder.
* **Optimisez les schémas de stockage de tables**. Quand vous utilisez des magasins de tables qui nécessitent que les noms de tables et de colonnes soient transmis et traités avec chaque requête, tels que le stockage de tables Azure, utilisez des noms plus courts pour réduire cette surcharge. Toutefois, ne sacrifiez pas la lisibilité ou la facilité de gestion en utilisant des noms trop compacts.
* **Utilisez la bibliothèque parallèle de tâches (TPL) pour effectuer des opérations asynchrones**. La bibliothèque parallèle de tâches (TPL, Task Parallel Library) facilite l’écriture de code asynchrone qui effectue des opérations liées aux E/S. Utilisez si possible *ConfigureAwait(false)* pour éliminer la dépendance d’une liaison dans un contexte de synchronisation spécifique. Cela réduit les risques de blocage de thread.
* **Créez des dépendances de ressources lors du déploiement ou au démarrage de l’application**. Évitez les appels répétés à des méthodes qui testent l’existence d’une ressource puis créent la ressource si elle n’existe pas. (Des méthodes telles que *CloudTable.CreateIfNotExists* et *CloudQueue.CreateIfNotExists* dans la bibliothèque cliente de stockage Azure suivent ce modèle.) Ces méthodes peuvent imposer une surcharge considérable si elles sont appelées avant chaque accès à une table de stockage ou à une file d’attente de stockage. Au lieu de cela :
  * Créez les ressources nécessaires quand l’application est déployée ou lors de son premier démarrage (un seul appel à *CreateIfNotExists* pour chaque ressource dans le code de démarrage pour un rôle de travail ou web est acceptable). Veillez toutefois à gérer les exceptions qui peuvent se présenter si votre code tente d’accéder à une ressource inexistante. En pareil cas, vous devez consigner l’exception et potentiellement prévenir un opérateur qu’il manque une ressource.
  * Dans certaines circonstances, il peut être judicieux de créer la ressource manquante dans le code de gestion des exceptions, mais vous devez adopter cette approche avec précaution, car l’inexistence de la ressource peut être symptomatique d’une erreur de programmation (un nom de ressource mal orthographié, par exemple) ou d’un autre problème au niveau de l’infrastructure.
* **Utilisez des infrastructures légères**. Choisissez soigneusement les API et les infrastructures que vous utilisez afin de réduire l’utilisation des ressources, le délai d’exécution et la charge globale sur l’application. Par exemple, l’utilisation de l’API web pour gérer les demandes de service peut réduire l’encombrement des applications et accélérer l’exécution, mais il se peut qu’elle ne convienne pas pour des scénarios avancés où les fonctionnalités supplémentaires de Windows Communication Foundation sont nécessaires.
* **Envisagez de réduire le nombre de comptes de service**. Par exemple, utilisez un compte spécifique pour accéder aux ressources ou services qui imposent un nombre limite de connexions ou offrent de meilleures performances avec un nombre réduit de connexions simultanées. Cette approche est courante pour les services tels que les bases de données, mais elle peut affecter la capacité à vérifier avec précision les opérations en raison de l’emprunt d’identité de l’utilisateur d’origine.
* **Procédez au profilage des performances et au test de charge** pendant le développement, dans le cadre des routines de test et avant la publication de la version finale pour vous assurer que l’application fonctionne et est mise à l’échelle en fonction des besoins. Ce test doit être exécuté sur le même type de matériel que la plateforme de production et avec les mêmes types et quantités de données et de charge utilisateur que l’application rencontrera en production. Pour plus d’informations, consultez [Test des performances d’un service cloud](vs-azure-tools-performance-profiling-cloud-services.md).




<!--HONumber=Nov16_HO3-->


